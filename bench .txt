vllm serve Qwen/Qwen3-4B --gpu-memory-utilization 0.98 --max-model-len 8192 --enforce-eager --port 8000

vllm bench serve \
  --backend vllm \
  --model Qwen/Qwen3-4B \
  --endpoint /v1/completions \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 100 \
  --result-dir "./log/" \
  --save-result 

--------------------------------------------------------------------------------------------------------------------------

vllm serve Qwen/Qwen3-4B-AWQ --gpu-memory-utilization 0.98 --max-model-len 8192 --enforce-eager --port 8000

vllm bench serve \
  --backend vllm \
  --model Qwen/Qwen3-4B-AWQ \
  --endpoint /v1/completions \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 100 \
  --result-dir "./log/" \
  --save-result 

--------------------------------------------------------------------------------------------------------------------------

VLLM_USE_V1=0 vllm serve JunHowie/Qwen3-4B-GPTQ-Int4 --quantization gptq --dtype float16 --gpu-memory-utilization 0.98 --max-model-len 8192 --enforce-eager --port 8000

vllm bench serve \
  --backend vllm \
  --model JunHowie/Qwen3-4B-GPTQ-Int4 \
  --endpoint /v1/completions \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 100 \
  --result-dir "./log/" \
  --save-result 